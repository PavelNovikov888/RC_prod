{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g9LYflcmsXI"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lf2_XP2nFLx"
      },
      "outputs": [],
      "source": [
        "! pip install scipy -q\n",
        "! pip install lightfm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa5EmH0V5xjl",
        "outputId": "9f79d213-57b3-42f1-e473-f4c917159d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.sparse import csr_matrix\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyLxEpYWstJg"
      },
      "source": [
        "# Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPlWxKbkatAO"
      },
      "outputs": [],
      "source": [
        "# Десериализация\n",
        "with open ('/content/model_base', 'rb') as fp:\n",
        "  model_base = pickle.load(fp)\n",
        "with open ('/content/model_items', 'rb') as fp:\n",
        "  model_items = pickle.load(fp)\n",
        "with open ('/content/model_users', 'rb') as fp:\n",
        "  model_users = pickle.load(fp)\n",
        "\n",
        "with open ('/content/df_all', 'rb') as fp:\n",
        "  df_all = pickle.load(fp)\n",
        "with open ('/content/user_attributes', 'rb') as fp:\n",
        "  user_attributes = pickle.load(fp)\n",
        "with open ('/content/train_interactions', 'rb') as fp:\n",
        "  train_interactions = pickle.load(fp)\n",
        "with open ('/content/inv_item_mappings', 'rb') as fp:\n",
        "  inv_item_mappings = pickle.load(fp)\n",
        "with open ('/content/inv_user_mappings', 'rb') as fp:\n",
        "  inv_user_mappings = pickle.load(fp)\n",
        "with open ('/content/item_metadata', 'rb') as fp:\n",
        "  item_metadata = pickle.load(fp)\n",
        "with open ('/content/item_metadata_list', 'rb') as fp:\n",
        "  item_metadata_list = pickle.load(fp)\n",
        "with open ('/content/user_metadata', 'rb') as fp:\n",
        "  user_metadata = pickle.load(fp)\n",
        "with open ('/content/user_metadata_mappings_users', 'rb') as fp:\n",
        "  user_metadata_mappings_users = pickle.load(fp)\n",
        "with open ('/content/user_mappings', 'rb') as fp:\n",
        "  user_mappings = pickle.load(fp)\n",
        "with open ('/content/item_mappings', 'rb') as fp:\n",
        "  item_mappings = pickle.load(fp)\n",
        "with open ('/content/train', 'rb') as fp:\n",
        "  train = pickle.load(fp)\n",
        "with open ('/content/item_metadata_mappings_items', 'rb') as fp:\n",
        "  item_metadata_mappings_items = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7shm04pCJdK",
        "outputId": "d102ab37-4c0a-461e-e1ce-178aabc72a37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: 4332\n",
            "По данному user имеется история взаимодействий с items.\n",
            "Рекомендации строятся на основе коллаборативной фильтрации\n",
            "Предыдущие покупки данного user:\n",
            "321054\n",
            "Top 3 recommendations:\n",
            "(461686, 0)\n",
            "(48030, 1)\n",
            "(7943, 2)\n",
            "Top 3 new items recommendations:\n",
            "(461686, 0)\n",
            "(48030, 1)\n",
            "(7943, 2)\n",
            "\n",
            "\n",
            "User: 860445\n",
            "По данному user нет данных о взаимодействии с items.\n",
            "Рекомендации строятся на основе матричного разложения\n",
            "Для построения рекомендаций сгенерируем случайные признаки user:\n",
            "['day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'Month_6', 'day_of_week_2']\n",
            "Рекомендации, сформированные на основе похожести признаков users\n",
            "   product_name  rank\n",
            "1        178480   617\n",
            "2         94371   660\n",
            "0         19278  1295\n",
            "\n",
            "User: 7574\n",
            "По данному user имеется история взаимодействий с items.\n",
            "Рекомендации строятся на основе коллаборативной фильтрации\n",
            "Предыдущие покупки данного user:\n",
            "29873\n",
            "Top 3 recommendations:\n",
            "(420960, 0)\n",
            "(7943, 1)\n",
            "(309778, 2)\n",
            "Top 3 new items recommendations:\n",
            "(420960, 0)\n",
            "(7943, 1)\n",
            "(309778, 2)\n",
            "\n",
            "\n",
            "User: 5238518\n",
            "По данному user нет данных о взаимодействии с items.\n",
            "Рекомендации строятся на основе матричного разложения\n",
            "Для построения рекомендаций сгенерируем случайные признаки user:\n",
            "['Day Period_Evening', 'Month_8', 'day_of_week_3', 'event_transaction', 'event_view']\n",
            "Рекомендации, сформированные на основе похожести признаков users\n",
            "   product_name  rank\n",
            "1        178480   617\n",
            "2         94371   660\n",
            "0         19278  1295\n",
            "\n"
          ]
        }
      ],
      "source": [
        "n=0\n",
        "while n<4:\n",
        "  if n%2 ==0:\n",
        "    user = random.choice(list(set(inv_user_mappings.keys())))\n",
        "    # Создадим матрицу всех пользователей и элементов, чтобы получить для них прогнозы\n",
        "    n_users, n_items = train_interactions.shape\n",
        "\n",
        "      # Используем lightFM to create predictions for all users and all items\n",
        "    scoring_user_ids = np.concatenate([np.full((n_items, ), i) for i in range(n_users)]) # повторим user ID для всех проб\n",
        "    scoring_item_ids = np.concatenate([np.arange(n_items) for i in range(n_users)]) # повторим весь диапазон идентификаторов item IDs x количество user\n",
        "    scores = model_base.predict(user_ids = scoring_user_ids,\n",
        "                                          item_ids = scoring_item_ids)\n",
        "    scores = scores.reshape(-1, n_items) # получим одну строку на каждого user\n",
        "      # Получим информацию о предыдущих покупках для каждого user\n",
        "    previous = np.array(train_interactions.todense())\n",
        "    print(f'User: {user}')\n",
        "    print(f'По данному user имеется история взаимодействий с items.\\nРекомендации строятся на основе коллаборативной фильтрации')\n",
        "    # 3 лучших прогнозов для каждого пользователя\n",
        "    k=3\n",
        "    top_3 = np.argsort(-scores, axis=1) [::, :k]\n",
        "    print(\"Предыдущие покупки данного user:\", *[inv_item_mappings.get(key) for key in np.array(range(previous.shape[1]))[previous[user]>0]], sep=\"\\n\")\n",
        "    print(\"Top 3 recommendations:\", *sorted(zip([inv_item_mappings.get(key) for key in top_3[user]], range(k)), key = lambda x: x[1]), sep=\"\\n\")\n",
        "    # Удалим ранее купленные items из прогнозов\n",
        "    top_3_new  = np.argsort(-(scores-(previous*999999)), axis=1)[::, :k] # вычтем предыдущие покупки из прогнозов\n",
        "    print(\"Top 3 new items recommendations:\", *sorted(zip([inv_item_mappings.get(key) for key in top_3_new[user]], range(k)), key = lambda x: x[1]), sep=\"\\n\")\n",
        "    print('\\n')\n",
        "  else:\n",
        "    user = random.randrange(10000000)\n",
        "    try:\n",
        "      п = previous[user]\n",
        "      print(f'User: {user}')\n",
        "      print(f'По данному user имеется история взаимодействий с items.\\nРекомендации строятся на основе коллаборативной фильтрации')\n",
        "      k=3\n",
        "      top_3 = np.argsort(-scores, axis=1) [::, :k]\n",
        "      print(\"Предыдущие покупки данного user:\", *[inv_item_mappings.get(key) for key in np.array(range(previous.shape[1]))[previous[user]>0]], sep=\"\\n\")\n",
        "      print(\"Top 3 recommendations:\", *sorted(zip([inv_item_mappings.get(key) for key in top_3[user]], range(k)), key = lambda x: x[1]), sep=\"\\n\")\n",
        "        # Удалим ранее купленные items из прогнозов\n",
        "      top_3_new  = np.argsort(-(scores-(previous*999999)), axis=1)[::, :k] # вычтем предыдущие покупки из прогнозов\n",
        "      print(\"Top 3 new items recommendations:\", *sorted(zip([inv_item_mappings.get(key) for key in top_3_new[user]], range(k)), key = lambda x: x[1]), sep=\"\\n\")\n",
        "      print('\\n')\n",
        "    except:\n",
        "      print(f'User: {user}')\n",
        "      print(f'По данному user нет данных о взаимодействии с items.\\nРекомендации строятся на основе матричного разложения')\n",
        "      print(f'Для построения рекомендаций сгенерируем случайные признаки user:')\n",
        "      new_user_attriutes = random.sample(list(user_metadata),k=5)\n",
        "      print(new_user_attriutes)\n",
        "      user_indexes = [user_metadata_mappings_users.get(key) for key in new_user_attriutes]\n",
        "      # Can either just weight each attribute equally\n",
        "      weights = 1/len(user_indexes) # weight each metadata equally\n",
        "      std_weights = [[weights] * len(new_user_attriutes)]\n",
        "\n",
        "      # Combine the indexes we want populating with their weights\n",
        "      new_user = np.zeros(len(user_metadata_mappings_users)) # create an empty array that will server as our dummy cold-user row\n",
        "      np.put(new_user, user_indexes, std_weights) # update the relevant metadata attributes with the desired weights\n",
        "\n",
        "      #  Now we can predict on this cold-user just like any other\n",
        "      cold_user_preds = model_users.predict(user_ids = 0,\n",
        "                                      item_ids = [*item_mappings.values()],\n",
        "                                      item_features = item_metadata_list,\n",
        "                                      user_features = scipy.sparse.csr_matrix(new_user))\n",
        "\n",
        "      cold_ranks = np.argsort(-cold_user_preds)[:3]\n",
        "      cold_ranks = pd.DataFrame(zip([*inv_item_mappings.values()], cold_ranks), columns = [\"product_name\", \"rank\"])\n",
        "      print('Рекомендации, сформированные на основе похожести признаков users')\n",
        "      print(cold_ranks.sort_values([\"rank\"])[:3])\n",
        "      print('')\n",
        "  n+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_ZbfkWLnFOX"
      },
      "source": [
        "## Items feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jVRfm-sEzGd",
        "outputId": "8d2f78c7-e5fc-47e5-bf0d-1dde32bef8fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Item: 340938\n",
            "По данному item нет данных о взаимодействии с userss.\n",
            "Рекомендации строятся на основе матричного разложения\n",
            "Для построения рекомендаций сгенерируем случайные признаки item:\n",
            "['159_519769', '283_30603', 'available_1', '888_1284577', '283_1128577']\n",
            "Вначале найдем похожие items\n",
            "        cosine  item_name\n",
            "5041  0.980899      89143\n",
            "3437  0.964362     122324\n",
            "2240  0.956632      47000\n",
            "6198  0.953952     371228\n",
            "3020  0.947665     129972\n",
            "2250  0.943244     379796\n",
            "5966  0.939649     358984\n",
            "7808  0.939256     162863\n",
            "2280  0.938702     244285\n",
            "2981  0.935320     128745\n",
            "Затем определим users  подходящих по предпочтениям для данного item\n",
            "      cold_ranking  user_id\n",
            "2139           1.0   311402\n",
            "9089           1.0  1307466\n",
            "6386           1.0   929721\n",
            "\n",
            "Item: 4041\n",
            "По данному item имеется история взаимодействий с users.\n",
            "Рекомендации строятся на основе коллаборативной фильтрации\n",
            "Наиболее подходящие пользователи для продукта 4041: \n",
            "1309591\n",
            "173056\n",
            "903847\n",
            "\n",
            "\n",
            "Item: 550671\n",
            "По данному item нет данных о взаимодействии с userss.\n",
            "Рекомендации строятся на основе матричного разложения\n",
            "Для построения рекомендаций сгенерируем случайные признаки item:\n",
            "['888_726612', '888_832471', '400_424566', '888_1187104', '888_1154859']\n",
            "Вначале найдем похожие items\n",
            "        cosine  item_name\n",
            "3333  0.785261     386607\n",
            "5356  0.784366     136095\n",
            "5951  0.776917     423357\n",
            "2097  0.776839     199018\n",
            "6437  0.776001     277361\n",
            "8900  0.775552     354982\n",
            "7767  0.766473     174878\n",
            "326   0.765369     168386\n",
            "9307  0.762699     153407\n",
            "7162  0.762104     374759\n",
            "Затем определим users  подходящих по предпочтениям для данного item\n",
            "      cold_ranking  user_id\n",
            "2793           1.0   405379\n",
            "9690           1.0  1387057\n",
            "8009           1.0  1153115\n",
            "\n",
            "Item: 6297\n",
            "По данному item имеется история взаимодействий с users.\n",
            "Рекомендации строятся на основе коллаборативной фильтрации\n",
            "Наиболее подходящие пользователи для продукта 6297: \n",
            "863219\n",
            "1212999\n",
            "393265\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "n=0\n",
        "while n<4:\n",
        "  if n%2 != 0:\n",
        "    itemid = random.choice(list(set(inv_item_mappings.keys())))\n",
        "    # Создадим матрицу всех пользователей и элементов, чтобы получить для них прогнозы\n",
        "    n_users, n_items = train_interactions.shape\n",
        "    scores = model_base.predict(np.arange(n_users), np.repeat(itemid, n_users))\n",
        "    # Получаем индексы пользователей, отсортированных по их вероятності взаимодействия с продуктом\n",
        "    top_users_indices = np.argsort(-scores)\n",
        "    print(f'Item: {itemid}')\n",
        "    print(f'По данному item имеется история взаимодействий с users.\\nРекомендации строятся на основе коллаборативной фильтрации')\n",
        "    # Получаем индексы пользователей, отсортированных по их вероятности взаимодействия с продуктом\n",
        "    top_users_indices = np.argsort(-scores)\n",
        "    top_users = [inv_user_mappings[idx] for idx in top_users_indices][:3]\n",
        "    print(\"Наиболее подходящие пользователи для продукта {}: \".format(itemid))\n",
        "    for user in top_users:\n",
        "        print(user)\n",
        "    print('\\n')\n",
        "  else:\n",
        "    itemid = random.randrange(1000000)\n",
        "    if itemid in set(inv_item_mappings.keys()):\n",
        "      # Создадим матрицу всех пользователей и элементов, чтобы получить для них прогнозы\n",
        "      n_users, n_items = train_interactions.shape\n",
        "      scores = model_base.predict(np.arange(n_users), np.repeat(itemid, n_users))\n",
        "      # Получаем индексы пользователей, отсортированных по их вероятності взаимодействия с продуктом\n",
        "      top_users_indices = np.argsort(-scores)\n",
        "      print(f'Item: {itemid}')\n",
        "      print(f'По данному item имеется история взаимодействий с users.\\nРекомендации строятся на основе коллаборативной фильтрации')\n",
        "      # Получаем индексы пользователей, отсортированных по их вероятности взаимодействия с продуктом\n",
        "      top_users_indices = np.argsort(-scores)\n",
        "      top_users = [inv_user_mappings[idx] for idx in top_users_indices][:3]\n",
        "      print(\"Наиболее подходящие пользователи для продукта {}: \".format(itemid))\n",
        "      for user in top_users:\n",
        "          print(user)\n",
        "      print('\\n')\n",
        "    else:\n",
        "      print(f'Item: {itemid}')\n",
        "      print(f'По данному item нет данных о взаимодействии с userss.\\nРекомендации строятся на основе матричного разложения')\n",
        "      print(f'Для построения рекомендаций сгенерируем случайные признаки item:')\n",
        "      # Получим indexes for the feature combinations we want to return embeddings for\n",
        "      new_item_attriutes = random.sample(list(item_metadata),k=5)\n",
        "      print(new_item_attriutes)\n",
        "      new_item_indexes = [item_metadata_mappings_items.get(key) for key in new_item_attriutes]\n",
        "      # new_item_indexes\n",
        "      print('Вначале найдем похожие items')\n",
        "      # Can just weight each attribute equally\n",
        "      weights = 1/len(new_item_indexes) # weight each metadata equally\n",
        "      std_weights = [[weights] * len(new_item_attriutes)]\n",
        "\n",
        "      new_item = np.zeros(len(item_metadata_mappings_items)) # create an empty array that will serve as our dummy cold-user row\n",
        "      np.put(new_item, new_item_indexes, std_weights) # update the relevant metadata attributes with the desired weights\n",
        "\n",
        "      # Convert it into a sparse matrix\n",
        "      cold_item_matrix = scipy.sparse.csr_matrix(new_item)\n",
        "\n",
        "      # Use LightFM to convert the matrix into embeddings\n",
        "      cold_item_bias, cold_item_embedding = model_items.get_item_representations(cold_item_matrix)\n",
        "      item_biases, item_embeddings  = model_items.get_item_representations(features = item_metadata_list)\n",
        "\n",
        "      # Находим похожие items\n",
        "      item_item_cold = pd.DataFrame(cosine_similarity(cold_item_embedding, item_embeddings).T, columns=([\"cosine\"]))\n",
        "      item_item_cold[\"item_name\"]=item_item_cold.index.to_series().map(inv_item_mappings)\n",
        "      print(item_item_cold.sort_values(by=\"cosine\", ascending=False)[:10])\n",
        "      print(\"Затем определим users  подходящих по предпочтениям для данного item\")\n",
        "      # Create all user and item matrix to get predictions for it\n",
        "      n_users, n_items = train_interactions.shape\n",
        "\n",
        "      # Force lightFM to create predictions for all users and all items\n",
        "      scoring_user_ids = np.concatenate([np.full((n_items, ), i) for i in range(n_users)]) # repeat user ID for number of prods\n",
        "      scoring_item_ids = np.concatenate([np.arange(n_items) for i in range(n_users)]) # repeat entire range of item IDs x number of user\n",
        "      scores = model_items.predict(user_ids = scoring_user_ids,\n",
        "                                          item_ids = scoring_item_ids)\n",
        "      scores = scores.reshape(-1, n_items) # get 1 row per user\n",
        "      recommendations = pd.DataFrame(scores)\n",
        "\n",
        "      # Extract the user and item representations\n",
        "      user_biases, user_embeddings  = model_items.get_user_representations()\n",
        "      # Create prediction score for our 'new' item\n",
        "      recommendations[\"cold_ranking\"] = ((user_embeddings @ cold_item_embedding.T + cold_item_bias).T + user_biases).T\n",
        "      # recommendations.rank(axis=1, ascending=False)  # Highest value gets ranked as 1 i.e. best rec\n",
        "      cold_rankings = recommendations.rank(axis=1, ascending=False)[[\"cold_ranking\"]]\n",
        "      # cold_rankings\n",
        "\n",
        "      # Add on users\n",
        "      cold_rankings[\"user_id\"]=cold_rankings.index.to_series().map(inv_user_mappings)\n",
        "      print(cold_rankings.sort_values(by=\"cold_ranking\")[:3])\n",
        "      print('')\n",
        "  n+=1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4965386,
          "sourceId": 8356157,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30732,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
